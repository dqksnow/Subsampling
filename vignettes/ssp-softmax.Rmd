---
title: "An Introduction to `ssp.softmax`"
output: rmarkdown::html_vignette
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{ssp-softmax}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

This vignette takes logistic
regression model as an example of glm to describe the usage of function
`ssp.glm`. The statistical theory and algorithms in this implementation are
described in references papers.

The log likelihood function of glm is

$$
\max_{\beta} L(\beta) = \frac{1}{N} \sum_{i=1}^N \left\{y_i u(\beta^{\top} x_i)
- \psi \left[ u(\beta^{\top} x_i) \right] \right\}.
$$
where $u$ and $\psi$ are known functions depend on the exponential family. For
binomial family, the log likelihood function becomes

$$
\max_{\beta} L(\beta) = \frac{1}{N} \sum_{i=1}^N \left[y_i \beta^{\top} x_i -
\log\left(1 + e^{\beta^\top x_i}\right) \right].
$$

The idea of subsampling methods is as follows: instead of fitting the model to
the size $N$ full dataset, we first assign a subsampling probability to each
observation in the full dataset. Then we draw a relative small subsample and fit
the model to this subsample. The sampling probabilities are assigned based on
the goal of making the subsample more informative.

# Basic Usage of `ssp.glm`

You can install the development version of subsampling from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("dqksnow/Subsampling")
```

```{r setup}
library(knitr)
library(subsampling)
```

We introduce the basic usage by using `ssp.glm` on a simulated example data. $X$
contains $d=6$ covariates which follow multinormal distribution and $Y$ is the
binary response variable. The full data size is $N = 1 \times 10^4$.

```{r}
set.seed(1)
N <- 1e4
beta0 <- rep(-0.5, 7)
d <- length(beta0) - 1
corr <- 0.5
sigmax  <- matrix(corr, d, d) + diag(1-corr, d)
X <- MASS::mvrnorm(N, rep(0, d), sigmax)
colnames(X) <- paste("V", 1:ncol(X), sep = "")
P <- 1 - 1 / (1 + exp(beta0[1] + X %*% beta0[-1]))
Y <- rbinom(N, 1, P)
data <- as.data.frame(cbind(Y, X))
formula <- Y ~ .
head(data)
```

The function usage is

```{r, eval = FALSE}
ssp.glm(
  formula,
  data,
  subset = NULL,
  n.plt,
  n.ssp,
  family = "quasibinomial",
  criterion = "optL",
  sampling.method = "poisson",
  likelihood = "weighted",
  control = list(...),
  contrasts = NULL,
  ...
  )
```

For clarity, we define the terms used in this vignette. The entire dataset we
have on hand is referred to as the "full dataset" and the estimator derived from
this dataset is called the "full data estimator". The sample we draw from the
full dataset is called the "subsample" and the corresponding estimator is the
"subsample estimator". The subsampling probability is denoted as $\pi$.

## Arguments

Arguments `criterion`, `sampling.method` and `likelihood` correspond to three
questions: 

- How to compute the sampling probability for each observation?

- How do subsamples be drawn? 

- How should the likelihood function be modified to
account for the bias introduced by the subsampling process?

### `criterion`

The choices of `criterion` include `optA`, `optL`(default), `LCC` and `uniform`. The optimal subsampling criterion `optA` and `optL` are derived by minimizing the asymptotic covariance of subsample estimator, proposed by @wang2018optimal. `LCC` and `uniform` are baseline methods.

### `sampling.method`

The options for the `sampling.method` argument include `withReplacement`
and `poisson` (default). `withReplacement.` stands for drawing $n.ssp$
subsamples from full dataset of size $N$ with replacement, using the specified
subsampling probability. `poisson` stands for drawing subsamples one by one by
comparing the subsampling probability with a realization of uniform random
variable $U(0,1)$. The expected number of drawed samples are $n.ssp$. More details see @wang2019more.

### `likelihood`

The available choices for `likelihood` include `weighted` (default) and
`logOddsCorrection`. Both of these likelihood functions can derive an unbiased optimal subsample
estimator. Theoretical results indicate that `logOddsCorrection` is more
efficient than `weighted` in the context of logistic regression. See
@wang2022maximum.

## Results

After drawing subsamples, `ssp.glm` utilizes `survey::svyglm` to fit the model
on the subsample, which eventually uses `glm`. Arguments used in `svyglm` can be
added in `ssp.glm` through `...` and will be passed to `svyglm`.

```{r}
n.plt <- 200
n.ssp <- 600
ssp.results <- ssp.glm(formula = formula,
                       data = data,
                       n.plt = n.plt,
                       n.ssp = n.ssp,
                       family = "quasibinomial",
                       criterion = "optL",
                       sampling.method = "poisson",
                       likelihood = "weighted"
                       )
summary(ssp.results)
```

```{r}
ssp.results <- ssp.glm(formula = formula,
                       data = data,
                       n.plt = n.plt,
                       n.ssp = n.ssp,
                       family = "quasibinomial",
                       criterion = "optA",
                       sampling.method = "poisson",
                       likelihood = "logOddsCorrection"
                       )
summary(ssp.results)
```

As recommended by `survey::svyglm`, for binomial family, please use
`family=quasibinomial()` to avoid a warning issued by `glm`. Refer to [svyglm() help
documentation Details
](https://www.rdocumentation.org/packages/survey/versions/4.4-2/topics/svyglm). The
'quasi' version of the family objects provide the same point estimates.

### Returned object 

`ssp.results` is an object contains estimation results and index of drawn
subsamples in the full dataset. 

```{r}
names(ssp.results)
```

- `index.plt` and `index` are the row index of
drawn pilot subsamples and optimal subsamples in the full data. They are ready
to be used for further analysis or downstream tasks.

- `coef.ssp`
is the optimal subsample estimator for $\beta$ and `coef` is the linear 
combination of `coef.plt` and `coef.ssp`. The coefficients and standard errors printed by summary are `coef` and the square root of `diag(cov)`.

- `cov.ssp` and `cov` are estimated covariance matrix of `coef.ssp` and
`coef`. 

- `subsample.size.expect` is the expected subsample size which is equals to
`n.ssp` when we use `ssp.glm`.

## Other Families

We also provide examples for poisson regression and gamma regression in the help documentation of `ssp.glm`. Note that currently `likelihood = logOddsCorrection` is implemented only for logistic regression (family = binomial or quasibonomial).

### References